import json
import os
import pandas as pd
from celery import Celery
from sqlalchemy import create_engine
from sqlalchemy import exc
from shutil import copy
import glob
import time
import smtplib
from email.mime.text import MIMEText

from app import app
from app import Configuration

import pandas as pd
import numpy as np
from sqlalchemy import create_engine
import math
import sys
import os
import timeit

import pandas as pd
import numpy as np
from sqlalchemy import create_engine
import math
import sys
import os
import timeit
from datetime import datetime, timedelta
from app.tasks.common_functions import fetch_db, misnomer_conversion,\
    check_in_std_cst, validate_pon, validate_depot, process_error_pon,\
    to_sql_customer_dna_record, read_sap_export_file, to_sql_sap_inventory,\
    add_hnad, to_sql_bom, read_data

from app.tasks.customer_dna import cleaned_dna_file

engine = create_engine(Configuration.INFINERA_DB_URL)
connection = Configuration.INFINERA_DB_URL


def make_celery(app):
    celery = Celery(app.import_name,
                    broker=app.config['CELERY_BROKER_URL'])
    celery.conf.update(app.config)
    TaskBase = celery.Task
    class ContextTask(TaskBase):
        abstract = True
        def __call__(self, *args, **kwargs):
            with app.app_context():
                return TaskBase.__call__(self, *args, **kwargs)
    celery.Task = ContextTask
    return celery

celery = make_celery(app)

def add_prospect(email_id):
    engine = create_engine(Configuration.INFINERA_DB_URL)
    selectquery = "select prospects_id FROM prospect_details where prospects_email='{0}'".format(email_id)
    result = engine.execute(selectquery).fetchone()
    if result is not None:
        return result[0]
    else:
        query = "insert into prospect_details (prospects_email) values('{0}')".format(email_id)
        engine.execute(query)
        result = engine.execute(selectquery).fetchone()
        return result[0]


def update_prospect_step(prospects_id, step_id, analysis_date):
    engine = create_engine(Configuration.INFINERA_DB_URL)
    try:
        selectquery = "select * FROM prospect_status where analysis_request_time='{0}'" \
                      "and prospects_id={1}".format(analysis_date, prospects_id)
        result = engine.execute(selectquery).fetchone()
        if result is None:
            query = "insert into prospect_status (prospects_id,prospects_step,analysis_request_time) values({0},{1},'{2}')".format(prospects_id,
                                                                                                 step_id, analysis_date)
            print(query)
            engine.execute(query)

        else:
            query = "update prospect_status set prospects_step = {0} where prospects_id = {1}" \
                    " and analysis_request_time='{2}'".format(step_id, prospects_id, analysis_date)
            print(query)
            engine.execute(query)

    except:
        print("Failed to update status for prospects_id {0}".format(prospects_id))



def shared_function(dna_file, sap_file):

    # 5.4 Load all Data elements from Reference Data
    (misnomer_pons, standard_cost, node, spared_pons, highspares, get_ratio_to_pon, parts,
     parts_cost, high_spares, depot) = fetch_db()

    # clean PONs, part# and installed equipments
    input_db = cleaned_dna_file(dna_file)


    # 5.5 Convert Misnomer PON to correct PON
    # PON with no misnomers
    input_with_no_misnomer_pon = misnomer_conversion(input_db, misnomer_pons)
    to_sql_customer_dna_record('customer_dna_record', input_db)

    print('Checking PON, part# in std cost...')
    # 5.6 Apply Standard Cost Rule
    input_with_no_misnomer_pon = check_in_std_cst(input_with_no_misnomer_pon, standard_cost)

    # Assign PON to Installed equipments to remove any ambiguity
    input_with_no_misnomer_pon['InstalledEqpt'] = input_with_no_misnomer_pon['Product Ordering Name']

    # 5.7 Validate PONs and Depot
    print(input_with_no_misnomer_pon.shape)
    valid_pon = validate_pon(input_with_no_misnomer_pon)
    valid_pon = validate_depot(valid_pon)
    print(valid_pon.shape)
    # 5.8 Identify Valid Sparable Items and Identify Error Conditions

    # 5.8.1 Calculate node_to_depot
    valid_pon = pd.merge(valid_pon, node, left_on='Node Name', right_on='node_name', how='left')

    # Add node_depot_belongs attribute ,If True PON has depot name
    valid_pon['has_node_depot'] = False
    valid_pon.loc[(valid_pon['node_depot_belongs'].notna()), 'has_node_depot'] = True

    # 5.8.2 Collect attributes A) PON Has Std Cost or Not
    valid_pon = pd.merge(valid_pon, standard_cost, left_on=['Product Ordering Name'], right_on=['part_name'],
                         how='left')
    # Remove unnecessary column generated by merge

    valid_pon = valid_pon.drop(['created_at', 'updated_at', 'Source'], 1)
    valid_pon['has_std_cost'] = False
    valid_pon.loc[(valid_pon['standard_cost'].notna()), 'has_std_cost'] = True

    # 5.8.2 Collect attributes - B) PON is Sparrable or Not
    valid_pon['is_sparrable'] = False
    valid_pon = pd.merge(valid_pon, spared_pons, left_on=['Product Ordering Name'], right_on=['part_name'], how='inner')

    valid_pon.loc[(valid_pon['Product Ordering Name'] == valid_pon['part_name_x']), 'is_sparrable'] = True


    # Remove unnecessary column generated by merge
    valid_pon = valid_pon.drop(['part_name_x', 'part_name_y'], 1)

    # Keep only sparable pons
    valid_pon = valid_pon[valid_pon['is_sparrable'] == True]

    # all_valid conditions
    # PON with sparable, has_std_cost,has_node_depot and valid pon_name & depot name
    all_valid = valid_pon[((valid_pon['has_std_cost'] == True) & (valid_pon['has_node_depot'] == True))]

    invalid_pon = valid_pon[~((valid_pon['has_std_cost'] == True) & (valid_pon['has_node_depot'] == True))]

    # 5.9 Process Error Records - Store Invalid PONS with Proper Reason
    process_error_pon('error_records', invalid_pon)

    # 5.9 Process Error Records - Compare Valid PON against

    sap_inventory = read_sap_export_file(sap_file)
    to_sql_sap_inventory('sap_inventory', sap_inventory.head(10))
    return all_valid, parts, get_ratio_to_pon, depot, high_spares, standard_cost


def bom_calcuation(dna_file, sap_file):


    all_valid, parts, get_ratio_to_pon, depot, high_spares, standard_cost = shared_function(dna_file, sap_file)
    Get_Fru = pd.DataFrame(
        all_valid.groupby(['Product Ordering Name', 'node_depot_belongs'])['node_depot_belongs'].count())
    Get_Fru.to_csv(Configuration.fruc_file_location, index=True)
    Get_Fru = pd.read_csv(Configuration.fruc_file_location)
    Get_Fru = Get_Fru.rename(columns={'node_depot_belongs.1': 'count'})
    Get_Fru.groupby(['Product Ordering Name', 'node_depot_belongs'])['count'].last().unstack(
        fill_value=0).stack().to_csv(Configuration.bom_table, header=True)
    get_bom_for_table = pd.read_csv(Configuration.bom_table)
    get_bom_for_table = get_bom_for_table.rename(columns={'0': 'PON Quanity'})
    get_bom_for_table.to_csv("/Users/anup/eKryp/infinera/Parts-Analysis/data/install_base.csv", index=False)
    return get_bom_for_table, get_ratio_to_pon, parts, depot, high_spares, standard_cost
    print('BOM calculation complete')


def simple_calculation(get_bom_for_table):
    get_bom_for_table.loc[get_bom_for_table['PON Quanity'] > 0, 'PON Quanity'] = 1
    get_bom_for_table.replace(np.nan, 0, inplace=True)
    get_bom_for_table = get_bom_for_table[['Product Ordering Name', 'node_depot_belongs', 'PON Quanity']]
    print("Simple Calculation")
    # print(get_bom_for_table)
    print("End Simple Calculation")
    return get_bom_for_table


def mtbf_calculation(get_bom_for_table, get_ratio_to_pon, parts):
    spare_cols = [str(x + 1) for x in range(10)]

    def find_spare_bin(row):
        bins = [0] + list(row[spare_cols])
        # Check for empyt count, if there is no calss family for PON
        # Then make required spared = 1, the logic is make the first column == FRU count
        # the return spare would be 1
        bins = [row['PON Quanity'] if math.isnan(x) else x for x in bins]
        return np.digitize(row['PON Quanity'], bins, right=True)

    Get_MTBF_BOM = pd.merge(get_bom_for_table, parts, left_on='Product Ordering Name', right_on='part_name', how='left')
    Get_MTBF_BOM = Get_MTBF_BOM[
        ['Product Ordering Name', 'node_depot_belongs', 'PON Quanity', 'part_reliability_class']]
    Get_MTBF_BOM = pd.merge(Get_MTBF_BOM, get_ratio_to_pon, left_on='part_reliability_class', right_on='product_family',
                            how='left')

    # If the reliability class is not found for a PON then we will assign a default count of 1
    Get_MTBF_BOM.loc[(Get_MTBF_BOM['part_reliability_class'].isna(), 'PON Quanity')] = 1

    Get_MTBF_BOM['PON Quanity'] = Get_MTBF_BOM.apply(find_spare_bin, axis=1)
    Get_MTBF_BOM_calc = Get_MTBF_BOM[['Product Ordering Name', 'node_depot_belongs', 'PON Quanity']]

    print("MTBF Calculation")
    # print(Get_MTBF_BOM_calc)
    print("End MTBF Calculation")
    return Get_MTBF_BOM_calc


def remove_hub_depot(df, depot):
    all_depots = pd.merge(df, depot, left_on='node_depot_belongs', right_on='depot_name', how='left')

    # PONs depots are HUB,hub=1 means they are HUB ,0 means they are not hub

    # Remove the PON which are in the below list
    # Make Pon quantity = 0 for PON in below list & their depot are not hub
    pon_list = ["DTC-ANSI-B=", "DTC-ETSI-B=", "OTC-ANSI-A=", "OTC-ETSI-A=", "XTC-2", "XTC-4",
                "XTC-10", "XTC-2E", "MTC-9-B", "MTC-6", "MPC-6", "MTC-9", "MTC-ANSI="]
    all_depots.loc[(all_depots['Product Ordering Name'].isin(pon_list)) & (all_depots['hub'] == 0), 'PON Quanity'] = 0

    # Keep only required columns
    all_depots = all_depots[['Product Ordering Name', 'node_depot_belongs', 'PON Quanity']]
    return all_depots


def calculate_shared_depot(single_bom, high_spares, standard_cost, parts):

    Connection = Configuration.ECLIPSE_DATA_DB_URI
    single_bom = pd.merge(single_bom, high_spares, on='part_name', how='left')
    get_zinventory_sql = 'SELECT storage_location,material_desc,total_stock,reorder_point FROM infinera.sap_inventory '
    z_inventory = read_data(get_zinventory_sql, Connection)
    single_bom = pd.merge(single_bom, z_inventory, left_on=['part_name', 'depot_name'],
                          right_on=['material_desc', 'storage_location'], how='left')

    # Initially make shared_quantiity for all PON as 0,
    single_bom['shared_quantity'] = 0
    single_bom['is_high_spares'] = False
    # For PON in bom whose pon_quantity > 0 and has not having high spares
    # make shared quantity as bom explosion quantity
    single_bom.loc[(single_bom['pon_quantity'] > 0) & (single_bom['high_spare'].isna()), 'shared_quantity'] = \
    single_bom['pon_quantity']

    # For PON in bom whose pon_quantity > 0 and has having high spares
    # and not present in zinventory
    # make shared quantity as bom explosion quantity

    single_bom.loc[(single_bom['pon_quantity'] > 0) & (single_bom['high_spare'].notnull()) & (
    single_bom['storage_location'].isna()), 'shared_quantity'] = single_bom['pon_quantity']

    # For PON in bom whose pon_quantity > 0 and has having high spares
    # and present in zinventory
    # check spared_part in present in same depot as bom & zinventory
    # make shared quantity as bom explosion quantity
    single_bom.loc[(single_bom['pon_quantity'] > 0) & (single_bom['high_spare'].notnull()) & (
    single_bom['storage_location'].notnull()) & (
                   single_bom['depot_name'] == single_bom['storage_location']), 'shared_quantity'] = single_bom[
        'pon_quantity']
    single_bom.loc[(single_bom['total_stock'].isna()), 'total_stock'] = 0
    single_bom.loc[(single_bom['reorder_point'].isna()), 'reorder_point'] = 0
    single_bom.loc[(single_bom['high_spare'].notnull()), 'is_high_spares'] = True

    single_bom = single_bom[
        ['part_name', 'depot_name', 'high_spare', 'total_stock', 'reorder_point', 'is_high_spares', 'shared_quantity']]
    single_bom['net_total_stock'] = single_bom['total_stock'] - single_bom['shared_quantity']
    single_bom['net_reorder_point'] = single_bom['reorder_point'] - single_bom['shared_quantity']

    # Get Part_number for summary table
    single_bom = pd.merge(single_bom, parts, on='part_name', how='left')
    single_bom = single_bom[['part_name', 'depot_name', 'total_stock', 'reorder_point',
                             'is_high_spares', 'shared_quantity', 'material_number', 'net_total_stock'
        , 'net_reorder_point']]

    # Get Part_Cost for summary table

    single_bom = pd.merge(single_bom, standard_cost, on='material_number', how='left')
    single_bom = single_bom[['part_name_x', 'depot_name', 'total_stock', 'reorder_point',
                             'is_high_spares', 'shared_quantity', 'material_number', 'net_total_stock'
        , 'net_reorder_point', 'standard_cost']]

    single_bom['net_total_stock_cost'] = single_bom['net_total_stock'] * single_bom['standard_cost']
    single_bom['net_reorder_point_cost'] = single_bom['net_reorder_point'] * single_bom['standard_cost']
    single_bom.rename(columns={
        'part_name_x': 'part_name',
        'is_high_spares': 'high_spare'
    }, inplace=True
    )
    single_bom['cust_id'] = 7
    single_bom.to_sql(name='summary', con=engine, index=False, if_exists='append')
    print("Loaded data into summary table")


def get_bom(dna_file, sap_file):

    bom, get_ratio_to_pon, parts, depot, high_spares, standard_cost = bom_calcuation(dna_file, sap_file)

    # Flag will be there to choose from simple or mtbf calculation.

    gross_depot = simple_calculation(bom)

    # 5.13 Accommodating for Corporate and Regional Disti - Process 13

    gross_depot = remove_hub_depot(gross_depot, depot)
    gross_depot_hnad = add_hnad(gross_depot, quantity=1)
    to_sql_bom('bom_calculated', gross_depot_hnad)
    return gross_depot_hnad, high_spares, standard_cost, parts

#@celery.task
def derive_table_creation(dna_file, sap_file, data_path, prospect_id, analysis_date, user_email_id):
    print(dna_file, sap_file)
    single_bom, high_spares, standard_cost, parts = get_bom(dna_file, sap_file)
    calculate_shared_depot(single_bom, high_spares, standard_cost, parts)

    def set_request_status_complete(analysis_date):
        engine = create_engine(Configuration.INFINERA_DB_URL)
        query = "update analysis_request set requestStatus='Completed' " \
                "where analysis_request_time = '{0}'".format(analysis_date)
        engine.execute(query)
    set_request_status_complete(analysis_date)






